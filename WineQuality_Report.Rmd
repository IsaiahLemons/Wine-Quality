---
title: "Capstone Project: Wine Quality"
author: "Isaiah Lemons"
date: "12/31/2019"
output: 
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This project is part of the HarvardX:PH125.9x Data Science Capstone project. There are two datasets used that provide multiple physiochemical tests based on red and white wine samples that came from northern Portugal. The goal of this project is to develop machine learning algorithms based on all the physiochemical test results provided, in attempt to predict if a certain wine will be of high quality. 

The datasets used can be found at the link below. 
(https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/).


## Data Wrangling

```{r load.packages, message=FALSE, warning=FALSE}
# Install any neccessary libraries
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
```

The following code was used to import the data and split it into test and training sets for later models.
```{r getting cleaning data, message=FALSE, warning=FALSE}
# Import the Red and White datasets 
url_red <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
url_white <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

red_data <- read.csv(url_red, sep=';')
white_data <- read.csv(url_white, sep=';')

# Merge the two separate red and white wine datasets into one dataset
wine <- rbind(red_data, white_data)

# Adding a column to classify an excellent wine quality
table(wine$quality)
wine <- wine %>% mutate(Excellent = ifelse(quality > 6, 1, 0))
wine$Excellent <- as.factor(wine$Excellent)

# remove files no longer necessary 
rm(url_red, url_white, red_data, white_data)

# Splitting data into test and train sets 80/20 split
set.seed(42)
#set.seed(1, sample.kind="Rounding") #if using R 3.5 or later
test_index <- createDataPartition(wine$Excellent, times = 1, p = 0.2, list = FALSE)
train_set <- wine[-test_index,]
test_set <- wine[test_index,]
```


## Exploratory Analysis
Once the data is available initial analysis and research the dataset can begin.
Using the code below we can see that this dataset is in tidy format, and it contains 6497 rows and 13 columns.
```{r exporation, message=FALSE, warning=FALSE}
# data is in tidy format
wine %>% as.tibble() 

# checking the structure of the data
str(wine)

# checking basic summary statistics 
summary(wine)

# Number of rows and columns
nrow(wine)  
ncol(wine) 

# Check for missing values
any(is.na(wine))
```

Installing additional libraries which may be useful for analysis and modeling
```{r load.additional.packages, message=FALSE, warning=FALSE}
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")

```

The first observation to check is the overall distribution of wines based on their quality.
```{r quality.distribution, message=FALSE, warning=FALSE}
# Overall Average Quality
mean(wine$quality)

# Distribution in Quality
wine %>% 
  ggplot(aes(quality)) + 
  geom_bar() +
  ggtitle("Distribution of Quality")

# Percentage of Excellent wines
mean(wine$Excellent == 1)
```


Based on the following code creating a heatmap can allow for pin-pointing a few attributes that have higher correlations than others. These variables may play a bigger part in predictions later on, so it's good to take a further look. (alcohol, total.sulfur.dioxide, free.sulfur.dioxide, residual.sugar, and density)
```{r correlation.plot, message=FALSE, warning=FALSE}
train.cor <- cor(subset(wine, select=-c(Excellent)))

ggplot(melt(train.cor), aes(Var1, Var2, fill=value)) +
  geom_tile(color = "white") + #color white is for border
  scale_fill_gradient2(low="blue", high="red", mid="white") +
  theme_minimal() + # minimal theme
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) + 
  coord_fixed()
```


The below density plots check each of the physichemical tests to give a better understanding of their distributions. The flag created during data wrangling portion which specifies if a wine was considered excellent or not can also be added to give further insights.
```{r density.plots, message=FALSE, warning=FALSE}
ggplot(wine, aes(alcohol, color=Excellent, fill=Excellent)) +
  geom_density(alpha = 0.4) 

ggplot(wine, aes(total.sulfur.dioxide, color=Excellent, fill=Excellent)) +
  geom_density(alpha = 0.4)

ggplot(wine, aes(chlorides, color=Excellent, fill=Excellent)) +
  geom_density(alpha = 0.4)

ggplot(wine, aes(volatile.acidity, color=Excellent, fill=Excellent)) +
  geom_density(alpha = 0.4)
  
ggplot(wine, aes(free.sulfur.dioxide, color=Excellent, fill=Excellent)) +
  geom_density(alpha = 0.4) 

ggplot(wine, aes(residual.sugar, color=Excellent, fill=Excellent)) +
  geom_density(alpha = 0.4)

ggplot(wine, aes(density, color=Excellent, fill=Excellent)) + 
  geom_density(alpha = 0.4)

ggplot(wine, aes(fixed.acidity, color=Excellent, fill=Excellent)) + 
  geom_density(alpha = 0.4)

ggplot(wine, aes(sulphates, color=Excellent, fill=Excellent)) + 
  geom_density(alpha = 0.4)

ggplot(wine, aes(citric.acid, color=Excellent, fill=Excellent)) + 
  geom_density(alpha = 0.4)

ggplot(wine, aes(pH, color=Excellent, fill=Excellent)) + 
  geom_density(alpha = 0.4)
```


Taking a look at a couple scatterplots may show how certain attributes directly correlate with the quality. As shown in the plots below, when the alcohol content increases the quality generally increases as well. Density however, the quality tends to decrease as density increases. 
```{r correlations, message=FALSE, warning=FALSE}
ggplot(wine, aes(alcohol, quality)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Quality by Alcohol")

ggplot(wine, aes(density, quality)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Quality by Density")
```


Stacked bar charts to easily show the percentage of excellent wines overall, based on particular variables. The images below continue to show similar results as previously mentioned, where higher quality wines tend to have higher alcohol content and lower densities. 
```{r stacked.bar, message=FALSE, warning=FALSE}
ggplot(wine, aes(alcohol, fill=Excellent)) + 
  geom_histogram(bins=30, position="fill")

ggplot(wine, aes(total.sulfur.dioxide, fill=Excellent)) +
  geom_histogram(bins=30, position="fill")

ggplot(wine, aes(free.sulfur.dioxide, fill=Excellent)) +
  geom_histogram(bins=30, position="fill")

ggplot(wine, aes(density, fill=Excellent)) + 
  geom_histogram(bins=30, position="fill")

ggplot(wine, aes(citric.acid, fill=Excellent)) + 
  geom_histogram(bins=30, position="fill")
```



## Modeling 
Each of the models below are using all the physiochemical tests variables available to predict if a wine will be considered excellent. The quality column is not included as a predictor of Excellent since it was directly used to create the classification flags.
```{r models, message=FALSE, warning=FALSE}
# Logistic Regression Model
set.seed(1)
train_glm <- train(Excellent ~ .-quality, method = "glm", data = train_set)
glm_pred <- predict(train_glm, test_set, type = "raw")
confusionMatrix(glm_pred, test_set$Excellent)$overall[["Accuracy"]]  


# LDA Model      
set.seed(1)
train_lda <- train(Excellent ~ .-quality, method = "lda", data = train_set)
lda_pred <- predict(train_lda, test_set)
confusionMatrix(lda_pred, test_set$Excellent)$overall[["Accuracy"]]  


# QDA Model     
set.seed(1)
train_lda <- train(Excellent ~ .-quality, method = "qda", data = train_set)
qda_pred <- predict(train_lda, test_set)
mean(lda_pred == test_set$Excellent) 


# Loess Model 
set.seed(1)
train_loess <- train(Excellent ~ .-quality, 
                     data = train_set,
                     method = "gamLoess")
loess_pred <- predict(train_loess, test_set)
mean(loess_pred == test_set$Excellent)


# K Nearest Neighbors Model
set.seed(1)
train_knn <- train(Excellent ~ .-quality, 
                   method = "knn", 
                   data = train_set,
                   tuneGrid = data.frame(k = seq(3,51,2)))
ggplot(train_knn, highlight = TRUE)  
train_knn$bestTune
knn_pred <- predict(train_knn, test_set, type = "raw")
confusionMatrix(knn_pred, test_set$Excellent)$overall["Accuracy"]


# Cross Validation Model
set.seed(1)
train_knn_cross <- train(Excellent ~ .-quality, 
                         method = "knn", 
                         data = train_set,
                         tuneGrid = data.frame(k = seq(3,51,2)),
                         trControl = trainControl(method = "cv", number = 10, p = .9))
ggplot(train_knn_cross, highlight = TRUE) 
train_knn_cross$bestTune
knn_cross_pred <- predict(train_knn_cross, test_set, type = "raw")
confusionMatrix(knn_cross_pred, test_set$Excellent)$overall["Accuracy"]


# Rpart
set.seed(1)
train_tree <- train(Excellent ~ .-quality, 
                    method = "rpart", 
                    data = train_set,
                    tuneGrid = data.frame(cp = seq(0, 0.05, 0.002)))
ggplot(train_tree, highlight = TRUE) 
train_tree$bestTune

rpart_pred <- predict(train_tree, test_set, type = "raw")
confusionMatrix(rpart_pred,test_set$Excellent)$overall["Accuracy"]

train_tree$finalModel
plot(train_tree$finalModel, margin = 0.1)
text(train_tree$finalModel)


# Random Forest Model
set.seed(1)
train_rf <- train(Excellent ~ .-quality, 
                  method = "rf", 
                  data = train_set,
                  tuneGrid = data.frame(mtry = seq(1:7)),
                  ntree = 100)
ggplot(train_rf, highlight = TRUE) 
train_rf$bestTune

rf_pred <- predict(train_rf, test_set, type = "raw")
confusionMatrix(rf_pred,test_set$Excellent)$overall["Accuracy"]
#importance of variables
varImp(train_rf)
```


## Results
```{r model.results, message=FALSE, warning=FALSE}
models <- c("Logistic Regression", "LDA", "QDA", "Loess", 
            "K nearest neighbors", "Cross Validation", "Rpart", "Random forest")
accuracy <- c(mean(glm_pred == test_set$Excellent), 
              mean(lda_pred == test_set$Excellent),
              mean(qda_pred == test_set$Excellent),
              mean(loess_pred == test_set$Excellent),
              mean(knn_pred == test_set$Excellent),
              mean(knn_cross_pred == test_set$Excellent),
              mean(rpart_pred == test_set$Excellent),
              mean(rf_pred == test_set$Excellent))
Model_Results <- data.frame(Model = models, Accuracy = accuracy)
Model_Results
```
When comparing the results from all the different models performance it is clear that the highest performing model was the Random Forest which had a 90.5% accuracy. This is large improvement from the lowest performing model which was QDA at 77.9%. 


## Conclusion 
Through our analysis we were able to interpret the visualizations and determine a few variables that play a higher role in predicting the outcome. These variables were later confirmed with the random forest model which showed both alcohol and density as having the highest importance for predictions. Since this data didn't contain every aspect of wine data, things like brand name, age, sell price could all potentially improve these models and make it easier to classify an excellent wine from a mediocre. 


